{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WacDZTrS2RE_"
      },
      "source": [
        "# Perceptron"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "_xcG1-W52NSe"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchvision import transforms, datasets\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2ccYr3R3cu-"
      },
      "source": [
        "### 1) Подготовка датасета"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "gK6qvX__3aH2"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1,), (0.1,))\n",
        "])\n",
        "\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2fenlE2Gj_Z"
      },
      "source": [
        "### 2) Нейронная сеть"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "vk0EpZ23Gnog"
      },
      "outputs": [],
      "source": [
        "class Perceptron(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Perceptron, self).__init__()\n",
        "    self.fc1 = nn.Linear(28 * 28, 256)\n",
        "    self.bn1 = nn.BatchNorm1d(256)\n",
        "    self.fc2 = nn.Linear(256, 128)\n",
        "    self.bn2 = nn.BatchNorm1d(128)\n",
        "    self.fc3 = nn.Linear(128, 10)\n",
        "    self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x.view(-1, 28 * 28)\n",
        "    x = self.bn1(torch.relu(self.fc1(x)))\n",
        "    x = self.dropout(x)\n",
        "    x = self.bn2(torch.relu(self.fc2(x)))\n",
        "    x = self.dropout(x)\n",
        "    x = self.fc3(x)\n",
        "    return x\n",
        "\n",
        "model = Perceptron().to('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=0.01)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SfmUnmabJYqL"
      },
      "source": [
        "### 3) Тестирование модели"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "wbo7QJt9JcL8"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, loader, device):\n",
        "  model.eval()\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  with torch.no_grad():\n",
        "    for images, labels in loader:\n",
        "      images, labels = images.to(device), labels.to(device)\n",
        "      outputs = model(images)\n",
        "      _, predicted = torch.max(outputs, 1)\n",
        "      total += labels.size(0)\n",
        "      correct += (predicted == labels).sum().item()\n",
        "  return correct / total"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbYroQSULHaW"
      },
      "source": [
        "### 4) Ранняя остановка"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "md4tgh_HLOUw"
      },
      "outputs": [],
      "source": [
        "class EarlyStopping:\n",
        "    def __init__(self, patience=5, min_delta=0):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.best_loss = None\n",
        "        self.counter = 0\n",
        "\n",
        "    def should_stop(self, val_loss):\n",
        "        if self.best_loss is None or val_loss < self.best_loss - self.min_delta:\n",
        "            self.best_loss = val_loss\n",
        "            self.counter = 0\n",
        "        else:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                return True\n",
        "        return False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QcCQiJSpXtf4"
      },
      "source": [
        "### 5) ModelCheckpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "m3iGBUvlXsbh"
      },
      "outputs": [],
      "source": [
        "class ModelCheckpoint:\n",
        "    def __init__(self, filepath, monitor='val_loss', mode='min', verbose=True):\n",
        "      self.filepath = filepath\n",
        "      self.monitor = monitor\n",
        "      self.mode = mode\n",
        "      self.best_score = None\n",
        "      self.verbose = verbose\n",
        "\n",
        "    def save_checkpoint(self, model, epoch, val_score):\n",
        "        if self.best_score is None or (\n",
        "            self.mode == 'min' and val_score < self.best_score or\n",
        "            self.mode == 'max' and val_score > self.best_score\n",
        "        ):\n",
        "            self.best_score = val_score\n",
        "            torch.save(model.state_dict(), self.filepath)\n",
        "            if self.verbose:\n",
        "                print(f\"✅ Модель сохранена на эпохе {epoch+1} с {self.monitor}: {val_score:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJl1iPoOLV0R"
      },
      "source": [
        "### 6) Тренировка и тестирование модели"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EuCXWaS9LRyI",
        "outputId": "34bfeec8-688d-4a51-bcf9-dead7aa1ce4d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Эпоха [1/50], Потери: 0.7503, Точность на валидации: 0.7365\n",
            "✅ Модель сохранена на эпохе 1 с val_loss: 0.7503\n",
            "Эпоха [2/50], Потери: 0.7465, Точность на валидации: 0.7535\n",
            "✅ Модель сохранена на эпохе 2 с val_loss: 0.7465\n",
            "Эпоха [3/50], Потери: 0.7461, Точность на валидации: 0.6984\n",
            "✅ Модель сохранена на эпохе 3 с val_loss: 0.7461\n",
            "Эпоха [4/50], Потери: 0.7480, Точность на валидации: 0.7690\n",
            "Эпоха [5/50], Потери: 0.7420, Точность на валидации: 0.7297\n",
            "✅ Модель сохранена на эпохе 5 с val_loss: 0.7420\n",
            "Эпоха [6/50], Потери: 0.7490, Точность на валидации: 0.7538\n",
            "Эпоха [7/50], Потери: 0.7464, Точность на валидации: 0.7039\n",
            "Эпоха [8/50], Потери: 0.7440, Точность на валидации: 0.7355\n",
            "Эпоха [9/50], Потери: 0.7441, Точность на валидации: 0.6839\n",
            "Эпоха [10/50], Потери: 0.7480, Точность на валидации: 0.7752\n",
            "Ранняя остановка сработала.\n",
            "Точность на тесте: 0.7297\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "early_stopping = EarlyStopping(patience=5)\n",
        "checkpoint = ModelCheckpoint(filepath='best_model.pth', monitor='val_loss', mode='min', verbose=True)\n",
        "\n",
        "\n",
        "for epoch in range(50):\n",
        "  model.train()\n",
        "  running_loss = 0.0\n",
        "  for images, labels in train_loader:\n",
        "    images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "    outputs = model(images)\n",
        "    loss = criterion(outputs, labels)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    running_loss += loss.item()\n",
        "\n",
        "  avg_train_loss = running_loss / len(train_loader)\n",
        "  val_accuracy = evaluate(model, test_loader, device)\n",
        "\n",
        "  print(f\"Эпоха [{epoch+1}/{50}], Потери: {avg_train_loss:.4f}, Точность на валидации: {val_accuracy:.4f}\")\n",
        "\n",
        "  checkpoint.save_checkpoint(model, epoch, avg_train_loss)\n",
        "\n",
        "  if early_stopping.should_stop(avg_train_loss):\n",
        "    print(\"Ранняя остановка сработала.\")\n",
        "    break\n",
        "\n",
        "model.load_state_dict(torch.load('best_model.pth'))\n",
        "\n",
        "test_accuracy = evaluate(model, test_loader, device)\n",
        "print(f\"Точность на тесте: {test_accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R9EU9htMU0dy"
      },
      "source": [
        "### 7) Tensorflow Keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "kxXdQ7YqUHL4"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Flatten, Input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.models import Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "thaFGF-rU-Jd",
        "outputId": "8cb07894-7447-4909-cbd1-81e4c4e57665"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m1494/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7105 - loss: 2.8838\n",
            "Epoch 1: val_loss improved from inf to 1.62541, saving model to best_model.h5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 9ms/step - accuracy: 0.7106 - loss: 2.8809 - val_accuracy: 0.7599 - val_loss: 1.6254\n",
            "Epoch 2/50\n",
            "\u001b[1m1494/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7198 - loss: 1.8699\n",
            "Epoch 2: val_loss improved from 1.62541 to 1.60891, saving model to best_model.h5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 8ms/step - accuracy: 0.7198 - loss: 1.8697 - val_accuracy: 0.7346 - val_loss: 1.6089\n",
            "Epoch 3/50\n",
            "\u001b[1m1496/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7167 - loss: 1.7701\n",
            "Epoch 3: val_loss improved from 1.60891 to 1.47928, saving model to best_model.h5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 8ms/step - accuracy: 0.7167 - loss: 1.7701 - val_accuracy: 0.8018 - val_loss: 1.4793\n",
            "Epoch 4/50\n",
            "\u001b[1m1497/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7161 - loss: 1.7397\n",
            "Epoch 4: val_loss improved from 1.47928 to 1.43482, saving model to best_model.h5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 8ms/step - accuracy: 0.7161 - loss: 1.7397 - val_accuracy: 0.8174 - val_loss: 1.4348\n",
            "Epoch 5/50\n",
            "\u001b[1m1498/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7207 - loss: 1.7100\n",
            "Epoch 5: val_loss did not improve from 1.43482\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 9ms/step - accuracy: 0.7207 - loss: 1.7100 - val_accuracy: 0.7898 - val_loss: 1.5105\n",
            "Epoch 6/50\n",
            "\u001b[1m1496/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7247 - loss: 1.6694\n",
            "Epoch 6: val_loss improved from 1.43482 to 1.38958, saving model to best_model.h5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 9ms/step - accuracy: 0.7247 - loss: 1.6694 - val_accuracy: 0.8109 - val_loss: 1.3896\n",
            "Epoch 7/50\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7215 - loss: 1.6568\n",
            "Epoch 7: val_loss did not improve from 1.38958\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 8ms/step - accuracy: 0.7215 - loss: 1.6568 - val_accuracy: 0.7671 - val_loss: 1.5284\n",
            "Epoch 8/50\n",
            "\u001b[1m1494/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7169 - loss: 1.6619\n",
            "Epoch 8: val_loss did not improve from 1.38958\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 8ms/step - accuracy: 0.7169 - loss: 1.6620 - val_accuracy: 0.8163 - val_loss: 1.4026\n",
            "Epoch 9/50\n",
            "\u001b[1m1499/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7166 - loss: 1.6800\n",
            "Epoch 9: val_loss did not improve from 1.38958\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 8ms/step - accuracy: 0.7166 - loss: 1.6799 - val_accuracy: 0.8005 - val_loss: 1.4670\n",
            "Epoch 10/50\n",
            "\u001b[1m1498/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7179 - loss: 1.6350\n",
            "Epoch 10: val_loss did not improve from 1.38958\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 8ms/step - accuracy: 0.7179 - loss: 1.6350 - val_accuracy: 0.7918 - val_loss: 1.3933\n",
            "Epoch 11/50\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7226 - loss: 1.6409\n",
            "Epoch 11: val_loss did not improve from 1.38958\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 9ms/step - accuracy: 0.7226 - loss: 1.6409 - val_accuracy: 0.7795 - val_loss: 1.5250\n",
            "Точность на тесте: 0.8122\n"
          ]
        }
      ],
      "source": [
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "\n",
        "x_train = x_train.reshape(-1, 28 * 28)\n",
        "x_test = x_test.reshape(-1, 28 * 28)\n",
        "\n",
        "y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, 10)\n",
        "\n",
        "def create_model():\n",
        "    inputs = Input(shape=(28 * 28,))\n",
        "\n",
        "    x = Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))(inputs)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "\n",
        "    x = Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "\n",
        "    outputs = Dense(10, activation='softmax')(x)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "model = create_model()\n",
        "model.compile(optimizer=Adam(learning_rate=0.01),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "model_checkpoint = ModelCheckpoint(\n",
        "    'best_model.h5',\n",
        "    monitor='val_loss',\n",
        "    save_best_only=True,\n",
        "    mode='min',\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "history = model.fit(\n",
        "    x_train, y_train,\n",
        "    epochs=50,\n",
        "    batch_size=32,\n",
        "    validation_split=0.2,\n",
        "    callbacks=[early_stopping, model_checkpoint],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "test_loss, test_accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
        "print(f\"Точность на тесте: {test_accuracy:.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
